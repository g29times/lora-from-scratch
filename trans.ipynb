{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def hello_world():\\n    print \"Hello, World!\"\\nhello_world()\\n\\n# Prompt user to enter a number and then calculate the sum and product\\ndef calculate_sum_product(num):\\n    sum = 0\\n    product = 1\\n    while num > 0:\\n        sum = sum + num % 10\\n        product = product * num // 10\\n        num //= 10\\n    return sum * product\\n\\n# Ask']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HF\n",
    "# https://zhuanlan.zhihu.com/p/576691638\n",
    "\n",
    "# Train\n",
    "# PEFT Megatron DeepSpeed https://zhuanlan.zhihu.com/p/619426866\n",
    "# https://github.com/liguodongiot/llm-action\n",
    "# https://huggingface.co/blog/zh/megatron-training\n",
    "# https://github.com/microsoft/Megatron-DeepSpeed\n",
    "# OpenMMLab FSDP ColossalAI DeepSpeed https://zhuanlan.zhihu.com/p/645564540\n",
    "# https://github.com/facebookresearch/llama-recipes/\n",
    "\n",
    "# Use\n",
    "# https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "# https://huggingface.co/docs/transformers/tasks/language_modeling#inference\n",
    "# https://boinc-ai.gitbook.io/transformers/api/main-classes/auto-classes/natural-language-processing/automodelforcausallm\n",
    "\n",
    "prompt = \"def hello_world():\"\n",
    "my_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# case 1 pipeline\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=my_model)\n",
    "# outputs = pipe(prompt)\n",
    "# print(outputs[0][\"generated_text\"])\n",
    "\n",
    "# case 2 AutoModelForCausalLM\n",
    "# Tokenize the text and return the input_ids as PyTorch tensors:\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(my_model)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# Use the generate() method to generate text.\n",
    "from transformers import AutoModelForCausalLM\n",
    "# pip install accelerate\n",
    "model = AutoModelForCausalLM.from_pretrained(my_model, device_map=\"auto\")\n",
    "inputs = inputs.to('cuda')\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "# Decode the generated token ids back into text:\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
